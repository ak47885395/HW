{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 範例：練習是從Wikipedia中爬取文章。先定義一個搜尋的關鍵字，擷取該關鍵字詞的文章。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re   #將網站轉換成 編碼後的url\n",
    "import os\n",
    "from urllib.request import urlretrieve "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 先定義一個我們想搜尋的字詞，並將它轉換成UTF-8編碼後的URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B'%E7%B6%B2%E8%B7%AF%E7%88%AC%E8%9F%B2'\n",
      "網路爬蟲: %E7%B6%B2%E8%B7%AF%E7%88%AC%E8%9F%B2\n",
      "/wiki/%E7%B6%B2%E8%B7%AF%E7%88%AC%E8%9F%B2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n1. repr\\n其產生的字串是給python的直譯器看的，這個字串會顯示'明確且教詳盡的資訊'\\n通常 可以讓python得知究竟這字串所代表的對象為何\\n2.幹 其實根本不用轉換\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_keyword=\"網路爬蟲\"  # define the word we wanna search\n",
    "\n",
    "url_utf8=repr(input_keyword.encode(\"UTF-8\")).upper()  #representation # 編碼成UTF-8並轉成大寫字元\n",
    "url_utf8 = url_utf8.replace(\"\\\\X\", \"%\") #replace \\x to %\n",
    "print(url_utf8)\n",
    "print(\"%s: %s\" % (input_keyword, url_utf8[2:-1:1]))     # 擷取中間的編碼結果\n",
    "\n",
    "# 組成Wiki關鍵字搜尋的網址格式\n",
    "root_keyword_link = '/wiki/' + url_utf8[2:-1:1]   # 家一下\n",
    "print(root_keyword_link)\n",
    "\n",
    "\n",
    "'''\n",
    "1. repr\n",
    "其產生的字串是給python的直譯器看的，這個字串會顯示'明確且教詳盡的資訊'\n",
    "通常 可以讓python得知究竟這字串所代表的對象為何\n",
    "2.幹 其實根本不用轉換\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 範例1：送出關鍵字請求後，爬取該關鍵字的文章內容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以防萬一 加個 headers\n",
    "headers={\n",
    "        'authority': 'zh.wikipedia.org' , \n",
    "'method': 'GET' ,\n",
    "'path':  root_keyword_link      # 好像要換掉喔\n",
    "}\n",
    "\n",
    "url = \"https://zh.wikipedia.org\" + root_keyword_link \n",
    "resp=requests.get(url , headers=headers)\n",
    "resp.encoding='UTF-8'\n",
    "html=BeautifulSoup(resp.text , \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "网络爬虫（英語：web crawler），也叫網路蜘蛛（spider），是一种用来自动浏览万维网的网络机器人。其目的一般为编纂网络索引（英语：Web indexing）。\n",
      "\n",
      "網路搜索引擎等站点通过爬蟲軟體更新自身的網站內容（英语：Web content）或其對其他網站的索引。網路爬蟲可以將自己所訪問的頁面保存下來，以便搜索引擎事後生成索引（英语：Index (search engine)）供用戶搜索。\n",
      "\n",
      "爬蟲访问网站的过程会消耗目标系统资源。不少网络系统并不默许爬虫工作。因此在访问大量页面时，爬虫需要考虑到规划、负载，还需要讲“礼貌”。 不愿意被爬虫访问、被爬虫主人知晓的公开站点可以使用robots.txt文件之类的方法避免访问。这个文件可以要求机器人（英语：Software agent）只对网站的一部分进行索引，或完全不作处理。\n",
      "\n",
      "互联网上的页面极多，即使是最大的爬虫系统也无法做出完整的索引。因此在公元2000年之前的万维网出现初期，搜索引擎经常找不到多少相关结果。现在的搜索引擎在这方面已经进步很多，能够即刻给出高质量结果。\n",
      "\n",
      "爬虫还可以验证超連結和HTML代码，用于网络抓取（英语：Web scraping）（参见数据驱动编程）。\n",
      "\n",
      "网络爬虫也可称作网络蜘蛛[1]、蚂蚁、自动索引程序（automatic indexer）[2] ，或（在FOAF（英语：FOAF (software)）软件中）称为网络疾走（web scutter）。[3]\n",
      "\n",
      "网络爬虫始于一张被称作种子的统一资源地址（URL）列表。当网络爬虫访问这些统一资源定位器时，它们会甄别出页面上所有的超链接，并将它们写入一张“待访列表”，即所谓爬行疆域（英语：crawl frontier）。此疆域上的URL将會被按照一套策略循环來访问。如果爬虫在执行的过程中复制归档和保存网站上的信息，这些档案通常储存，使他们可以較容易的被查看。阅读和浏览他们儲存的网站上並即时更新的信息，這些被儲存的網頁又被稱為“快照”。越大容量的網頁意味着网络爬虫只能在给予的时间内下载越少部分的网页，所以要优先考虑其下载。高变化率意味着网页可能已经被更新或者被取代。一些服务器端软件生成的URL（统一资源定位符）也使得网络爬虫很难避免检索到重复内容。\n",
      "\n",
      "但是互联网的资源卷帙浩繁，这也意味着网络爬虫只能在一定时间内下载有限数量的网页，因此它需要衡量優先順序的下载方式。有時候網頁出現、更新和消失的速度很快，也就是說网络爬虫下载的网页在幾秒後就已经被修改或甚至删除了。这些都是网络爬虫设计師们所面临的两个问题。\n",
      "\n",
      "再者，服务器端软件所生成的统一资源地址数量庞大，以至网络爬虫难免也會采集到重复的内容。根据超文本傳輸協定，无尽组合的参数所返回的页面中，只有很少一部分确实传回正確的内容。例如：數張快照陈列室的网站，可能通过几个参数，让用户选择相关快照：其一是通过四种方法对快照排序，其二是关于快照分辨率的的三种选择，其三是两种文件格式，另加一个用户可否提供内容的选择，这样对于同样的结果会有48种（4*3*2）不同的统一资源地址与其关联。这种数学组合替网络爬虫造成了麻烦，因为它们必须越过这些无关脚本变化的组合，寻找不重复的内容。\n",
      "\n",
      "爬虫的实现由以下策略组成：[4]\n",
      "\n",
      "爬虫可能只想搜索HTML页面而避免其他MIME 类型。为了只请求HTML资源，爬虫在抓取整个以GET方式请求的资源之前，通过创建HTTP的HEAD请求来决定网络资源的MIME类型。为了避免发出过多的请求，爬虫会检查URL和只请求那些以某些字符（如.html, .htm, .asp, .aspx, .php, .jsp, .jspx 或 / ）作为后缀的URL。这个策略可能会跳过很多HTML网络资源。\n",
      "\n",
      "有些爬虫还能避免请求一些带有“?”的资源（动态生成）。为了避免掉入从网站下载无限量的URL的爬虫陷阱。不过假若网站重写URL以简化URL的目的，这个策略就变得不可靠了。\n",
      "\n",
      "爬虫通常使用某些URL规范化的方式以避免资源的重复爬取。URL规范化，指的是以某种一致的方式修改和标准化URL的过程。这个过程有各种各样的处理规则，包括统一转换为小写、移除“.”和“..”片段，以及在非空路径里插入斜杆。\n",
      "\n",
      "有些爬虫希望从指定的网站中尽可能地爬取资源。而路径上移爬虫就是为了能爬取每个URL里提示出的每个路径。[5] 例如，给定一个Http的种子URL: http://llama.org/hamster/monkey/page.html ，要爬取 /hamster/monkey/ ， /hamster/ 和 / 。Cothey发现路径能非常有效地爬取独立的资源，或以某种规律无法在站内链接爬取到的资源。\n",
      "\n",
      "对于爬虫来说，一个页面的重要性也可以说是，给定查询条件一个页面相似性能起到的作用。网络爬虫要下载相似的网页被称为主题爬虫或局部爬虫。这个主题爬虫或局部爬虫的概念第一次被Filippo Menczer[6][7] 和  Soumen Chakrabarti 等人提出的。[8]\n",
      "\n",
      "网站的属性之一就是经常动态变化，而爬取网站的一小部分往往需要花费几个星期或者几个月。等到网站爬虫完成它的爬取，很多事件也已经发生了，包括增加、更新和删除。\n",
      "在搜索引擎的角度，因为没有检测这些变化，会导致存储了过期资源的代价。最常用的估价函数是新鲜度和过时性。\n",
      "新鲜度：这是一个衡量抓取内容是不是准确的二元值。在时间t内，仓库中页面p的新鲜度是这样定义的：\n",
      "\n",
      "过时性:这是一个衡量本地已抓取的内容过时程度的指标。在时间t时，仓库中页面p的时效性的定义如下：\n",
      "\n",
      "爬虫相比于人，可以有更快的检索速度和更深的层次，所以，他们可能使一个站点瘫痪。不需要说一个单独的爬虫一秒钟要执行多条请求，下载大的文件。一个服务器也会很难响应多线程爬虫的请求。\n",
      "就像Koster所注意的那样，爬虫的使用对很多工作都是很有用的，但是对一般的社区，也需要付出代价。使用爬虫的代价包括：[9]\n",
      "\n",
      "对这些问题的局部解决方法是漫游器排除协议（Robots exclusion protocol），也被称为robots.txt议定书[10]，这份协议是让管理员指明网络服务器的不应该爬取的约定。这个标准没有包括重新访问一台服务器的间隔的建议，虽然设置访问间隔是避免服务器超载的最有效办法。最近的商业搜索引擎，如Google，Ask Jeeves，MSN和Yahoo可以在robots.txt中使用一个额外的 “Crawl-delay”参数来指明请求之间的延迟。\n",
      "\n",
      "一个并行爬虫是并行运行多个进程的爬虫。它的目标是最大化下载的速度，同时尽量减少并行的开销和下载重复的页面。为了避免下载一个页面两次，爬虫系统需要策略来处理爬虫运行时新发现的URL，因为同一个URL地址，可能被不同的爬虫进程抓到。\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#爬取該關鍵字的文章內容\n",
    "content=html.find('div',attrs={\"id\":\"mw-content-text\"}).find_all(\"p\")\n",
    "#print(content.get_text())   it does not work cause it needs A string to convert I guess\n",
    "for paragraph in content:\n",
    "    print(paragraph.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 範例2：從爬取的文章內容中，擷取出有外部連結的關鍵字。這些關鍵字在文章中是以藍色字體顯示，會連到外部的網頁，並解釋其內容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "外部連結: 万维网 /wiki/%E4%B8%87%E7%BB%B4%E7%BD%91\n",
      "外部連結: 网络机器人 /wiki/%E7%BD%91%E7%BB%9C%E6%9C%BA%E5%99%A8%E4%BA%BA\n",
      "外部連結: 网络搜索引擎 /wiki/%E7%BD%91%E7%BB%9C%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E\n",
      "外部連結: 搜索引擎 /wiki/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E\n",
      "外部連結: Robots.txt /wiki/Robots.txt\n",
      "外部連結: 网站 /wiki/%E7%BD%91%E7%AB%99\n",
      "外部連結: 超連結 /wiki/%E8%B6%85%E9%80%A3%E7%B5%90\n",
      "外部連結: HTML /wiki/HTML\n",
      "外部連結: 数据驱动编程 /wiki/%E6%95%B0%E6%8D%AE%E9%A9%B1%E5%8A%A8%E7%BC%96%E7%A8%8B\n",
      "外部連結: 網頁 /wiki/%E7%B6%B2%E9%A0%81\n",
      "外部連結: 互联网 /wiki/%E4%BA%92%E8%81%94%E7%BD%91\n",
      "外部連結: 服务器 /wiki/%E6%9C%8D%E5%8A%A1%E5%99%A8\n",
      "外部連結: 超文本傳輸協定 /wiki/%E8%B6%85%E6%96%87%E6%9C%AC%E5%82%B3%E8%BC%B8%E5%8D%94%E5%AE%9A\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n1.這邊  如何 擷取 a的屬性是 重點\\n因為 href 會變化 需要用到 正規表達式\\n2.(\"^(/wiki/)((?!;)\\\\S)*$\")\\n^ 表示  取以 xxx 為開頭的 string  不過其實有許多意思    例如：/^A/ 不會匹配「an A」的 A，但會匹配「An E」中的 A。  \\n$  \\'\\'   取以 xxx 結尾的 string /t$/ 不會匹配「eater」中的 t，卻會匹配「eat」中的 t。  \\n\\\\S      不讀取 空白以後的東西\\n(?!;)   後面不是 ; 的    ^(/wiki/)\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for ex_link in content:\n",
    "    ex_tag= ex_link.find_all(\"a\", href=re.compile(\"^(/wiki/)((?!;)\\S)*$\"))   #  不建議 .a 一個 <P> 裡可能有好幾個\n",
    "#    print(ex_tag) \n",
    "#    test=ex_link.find_all(\"a\", href=re.compile(\"^(/wiki/)\"))   \n",
    "#    print(test)\n",
    "#  效果同上\n",
    "    for  link_string in ex_tag:\n",
    "         print(\"外部連結:\" ,link_string['title'] , link_string['href'])\n",
    "'''\n",
    "1.這邊  如何 擷取 a的屬性是 重點\n",
    "因為 href 會變化 需要用到 正規表達式\n",
    "2.(\"^(/wiki/)((?!;)\\S)*$\")\n",
    "^ 表示  取以 xxx 為開頭的 string  不過其實有許多意思    例如：/^A/ 不會匹配「an A」的 A，但會匹配「An E」中的 A。  \n",
    "$  ''   取以 xxx 結尾的 string /t$/ 不會匹配「eater」中的 t，卻會匹配「eat」中的 t。  \n",
    "\\S      不讀取 空白以後的東西\n",
    "(?!;)   後面不是 ; 的    ^(/wiki/)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 大師程式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "外部連結: [万维网] /wiki/%E4%B8%87%E7%BB%B4%E7%BD%91\n",
      "外部連結: [网络机器人] /wiki/%E7%BD%91%E7%BB%9C%E6%9C%BA%E5%99%A8%E4%BA%BA\n",
      "外部連結: [網路] /wiki/%E7%BD%91%E7%BB%9C%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E\n",
      "外部連結: [搜索引擎] /wiki/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E\n",
      "外部連結: [robots.txt] /wiki/Robots.txt\n",
      "外部連結: [网站] /wiki/%E7%BD%91%E7%AB%99\n",
      "外部連結: [超連結] /wiki/%E8%B6%85%E9%80%A3%E7%B5%90\n",
      "外部連結: [HTML] /wiki/HTML\n",
      "外部連結: [数据驱动编程] /wiki/%E6%95%B0%E6%8D%AE%E9%A9%B1%E5%8A%A8%E7%BC%96%E7%A8%8B\n",
      "外部連結: [網頁] /wiki/%E7%B6%B2%E9%A0%81\n",
      "外部連結: [互联网] /wiki/%E4%BA%92%E8%81%94%E7%BD%91\n",
      "外部連結: [服务器] /wiki/%E6%9C%8D%E5%8A%A1%E5%99%A8\n",
      "外部連結: [超文本傳輸協定] /wiki/%E8%B6%85%E6%96%87%E6%9C%AC%E5%82%B3%E8%BC%B8%E5%8D%94%E5%AE%9A\n"
     ]
    }
   ],
   "source": [
    "for ext_link in content:\n",
    "    a_tag = ext_link.find_all('a', href=re.compile(\"^(/wiki/)((?!;)\\S)*$\"))\n",
    "    if len(a_tag) > 0:\n",
    "        for link_string in a_tag:\n",
    "            a_link = link_string[\"href\"]       # 外部連結的網址\n",
    "            a_keyword = link_string.get_text()  # 外部連結的中文名稱\n",
    "            print(\"外部連結: [%s] %s\" % (a_keyword, a_link))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作業：接下來定義一個爬蟲函數，這個函數的主要工作為：\n",
    "(1) 爬取當前關鍵字的解釋，並存入檔案(因為文章內容太多會佔滿整個頁面，所以存程檔案，方便後續檢視)\n",
    "\n",
    "(2) 萃取出當前關鍵字所引用的外部連結，當作新的查詢關鍵字\n",
    "\n",
    "(3) 把第(2)擷取到的關鍵字當作新的關鍵字，回到第(1)步，爬取新的關鍵字解釋。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WikiArticle(key_word_link, key_word, recursive):\n",
    "     if (recursive <= max_recursive_depth):\n",
    "        print(\"遞迴層[%d] -%s (%s)\" % (recursive, key_word_link, key_word)) \n",
    "        \n",
    "        headers = {\n",
    "            'authority': 'zh.wikipedia.org',\n",
    "            'method': 'GET',\n",
    "            'path': '/wiki/' + key_word_link,\n",
    "            'scheme': 'https',\n",
    "            'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3',\n",
    "            'accept-encoding': 'gzip, deflate, br',\n",
    "            'accept-language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7,zh-CN;q=0.6',\n",
    "            'cookie': 'GeoIP=TW:TPE:Taipei:25.05:121.53:v4; TBLkisOn=0; mwPhp7Seed=8b8; WMF-Last-Access-Global=04-Jun-2019; WMF-Last-Access=04-Jun-2019',\n",
    "            'dnt': '1',\n",
    "            #'if-modified-since': 'Tue, 04 Jun 2019 12:03:22 GMT',\n",
    "            'referer': 'https://zh.wikipedia.org/wiki/Wikipedia:%E9%A6%96%E9%A1%B5',\n",
    "            'upgrade-insecure-requests': '1',\n",
    "            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'\n",
    "        }    \n",
    "        '''\n",
    "        ---------------以下為查詢關鍵字與擷取,儲存內容------------------\n",
    "        '''\n",
    "        url = 'https://zh.wikipedia.org' + key_word_link  # 組合關鍵字查詢URL\n",
    "        resp = requests.get(url, headers=headers)\n",
    "        resp.encoding = 'utf-8'\n",
    "\n",
    "        html = BeautifulSoup(resp.text, \"lxml\")\n",
    "        content = html.find(name='div', attrs={'id':'mw-content-text'}).find_all(name='p')\n",
    "        #\n",
    "        # Part 1: 請參考範例1，爬取當前關鍵字的文章內容。\n",
    "        #         因為內容太多，我們把它寫入檔案，並以關鍵字作為檔案名稱，以便稍後查閱內容。\n",
    "        #         請先建立一個名為\"WikiArticle\"的資料夾，爬取到的文章內容會放在這個資料夾底下。\n",
    "        #\n",
    "       # out_dir='WikiArticle'\n",
    "        if not os.path.exists('./WikiArticle/'):\n",
    "            os.makedirs( './WikiArticle/')   #os.makedirs( './Data' ,exist_ok =True )\n",
    "    #    with open ('./WikiArticle/WikiArticle.txt' , 'w' , encoding='utf-8') as  data:\n",
    "        key_word=key_word.replace(\"/\", \"_\");\n",
    "    #    print(key_word)\n",
    "        f = open('./WikiArticle/'+key_word+'.txt', \"w\",encoding=\"utf-8\")    \n",
    "        for string in content:\n",
    "                f.write(string.text)\n",
    "        f.close()\n",
    "        '''\n",
    "        --------------儲存成dict 以便使用 recursive -----------\n",
    "        1.特別注意 key_word=key_word.replace(\"/\", \"_\");\n",
    "        檔名的設定要特別注意\n",
    "        不能包括\n",
    "        \\/>?  ..  etc.\n",
    "        '''\n",
    "        #\n",
    "        # Part 2: 請參考範例2，萃取出本篇文章中所延伸引用的外部連結，並儲存在external_link_dict\n",
    "        #\n",
    "        external_link_dict = dict({})\n",
    "        for i in content:\n",
    "            a_tag=i.find_all( (\"a\") , href=re.compile(\"^/wiki/\"))\n",
    "     #       if len(a_tag) > 0:\n",
    "            for ex_url in a_tag:\n",
    "                    ex_link=ex_url['href']\n",
    "                    ex_title=ex_url['title']\n",
    "                    external_link_dict[ ex_link ] = ex_title\n",
    "              #  print(ex_title , \":\",\"https://zh.wikipedia.org\"+ex_link)\n",
    "#        data.close()\n",
    "       # Part 3: 將Part 2所收集的外部連結，當作新的關鍵字，繼續迭代深入爬蟲\n",
    "        #\n",
    "        if (len(external_link_dict) > 0):\n",
    "            \n",
    "            recursive = recursive + 1  # 遞迴深度加1\n",
    "            \n",
    "            for k, v in external_link_dict.items():   # 只是把  \"\" : \" \"拿掉((  K , v  只是隨便的變數 這邊用上了ex_link_dict中的數據\n",
    "                WikiArticle(k, v, recursive)  # 再次呼叫同樣的函數，執行同樣的流程 '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "遞迴層[0] -/wiki/%E7%B6%B2%E8%B7%AF%E7%88%AC%E8%9F%B2 (網路爬蟲)\n",
      "遞迴層[1] -/wiki/%E4%B8%87%E7%BB%B4%E7%BD%91 (全球資訊網)\n",
      "遞迴層[1] -/wiki/%E7%BD%91%E7%BB%9C%E6%9C%BA%E5%99%A8%E4%BA%BA (網路機器人)\n",
      "遞迴層[1] -/wiki/%E7%BD%91%E7%BB%9C%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E (網路搜尋引擎)\n",
      "遞迴層[1] -/wiki/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E (搜尋引擎)\n",
      "遞迴層[1] -/wiki/Robots.txt (Robots.txt)\n",
      "遞迴層[1] -/wiki/%E7%BD%91%E7%AB%99 (網站)\n",
      "遞迴層[1] -/wiki/%E8%B6%85%E9%80%A3%E7%B5%90 (超連結)\n",
      "遞迴層[1] -/wiki/HTML (HTML)\n",
      "遞迴層[1] -/wiki/%E6%95%B0%E6%8D%AE%E9%A9%B1%E5%8A%A8%E7%BC%96%E7%A8%8B (資料驅動編程)\n",
      "遞迴層[1] -/wiki/%E7%B6%B2%E9%A0%81 (網頁)\n",
      "遞迴層[1] -/wiki/%E4%BA%92%E8%81%94%E7%BD%91 (網際網路)\n",
      "遞迴層[1] -/wiki/%E6%9C%8D%E5%8A%A1%E5%99%A8 (伺服器)\n",
      "遞迴層[1] -/wiki/%E8%B6%85%E6%96%87%E6%9C%AC%E5%82%B3%E8%BC%B8%E5%8D%94%E5%AE%9A (超文字傳輸協定)\n"
     ]
    }
   ],
   "source": [
    "# 定義爬取的遞迴深度。深度不要訂太深，否則會爬很久。\n",
    "max_recursive_depth = 1\n",
    "\n",
    "WikiArticle(root_keyword_link, input_keyword, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
